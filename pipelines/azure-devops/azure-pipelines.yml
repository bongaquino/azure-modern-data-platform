# Azure DevOps CI/CD Pipeline for Modern Data Platform
# Implements automated deployment with comprehensive testing and validation

name: Modern Data Platform CI/CD
trigger:
  branches:
    include:
      - main
      - develop
      - feature/*
  paths:
    include:
      - infrastructure/*
      - pipelines/*
      - automation/*

pr:
  branches:
    include:
      - main
      - develop
  paths:
    include:
      - infrastructure/*
      - pipelines/*
      - automation/*

variables:
  # Global Variables
  - name: azureSubscription
    value: 'azure-data-platform-connection'
  - name: terraformVersion
    value: '1.6.0'
  - name: pythonVersion
    value: '3.9'
  - name: databricksCliVersion
    value: '0.18.0'
  
  # Environment-specific variable groups
  - group: 'dev-variables'
  - group: 'prod-variables'

stages:
  # ===========================
  # BUILD AND VALIDATE STAGE
  # ===========================
  - stage: BuildAndValidate
    displayName: 'Build and Validate'
    jobs:
      - job: InfrastructureValidation
        displayName: 'Infrastructure Validation'
        pool:
          vmImage: 'ubuntu-latest'
        
        steps:
          - checkout: self
            fetchDepth: 0
          
          # Install Terraform
          - task: TerraformInstaller@0
            displayName: 'Install Terraform'
            inputs:
              terraformVersion: $(terraformVersion)
          
          # Install Azure CLI
          - task: AzureCLI@2
            displayName: 'Install Azure CLI Extensions'
            inputs:
              azureSubscription: $(azureSubscription)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                az extension add --name databricks
                az extension add --name datafactory
                az extension add --name storage-preview
          
          # Terraform Format Check
          - script: |
              terraform fmt -check -recursive infrastructure/terraform/
            displayName: 'Terraform Format Check'
            continueOnError: false
          
          # Terraform Validation
          - script: |
              cd infrastructure/terraform
              terraform init -backend=false
              terraform validate
            displayName: 'Terraform Validation'
          
          # Terraform Security Scan with Checkov
          - script: |
              pip install checkov
              checkov -d infrastructure/terraform --framework terraform --output cli --output junitxml --output-file-path console,results.xml
            displayName: 'Infrastructure Security Scan'
            continueOnError: true
          
          # Publish Security Scan Results
          - task: PublishTestResults@2
            condition: always()
            inputs:
              testResultsFormat: 'JUnit'
              testResultsFiles: 'results.xml'
              testRunTitle: 'Infrastructure Security Scan'
              failTaskOnFailedTests: false

      - job: DataPipelineValidation
        displayName: 'Data Pipeline Validation'
        pool:
          vmImage: 'ubuntu-latest'
        
        steps:
          - checkout: self
          
          # Setup Python
          - task: UsePythonVersion@0
            inputs:
              versionSpec: $(pythonVersion)
              displayName: 'Setup Python'
          
          # Install Dependencies
          - script: |
              python -m pip install --upgrade pip
              pip install databricks-cli pytest great-expectations pyspark
            displayName: 'Install Dependencies'
          
          # Validate Databricks Notebooks
          - script: |
              python -m py_compile pipelines/databricks/notebooks/*.py
            displayName: 'Validate Databricks Notebooks'
          
          # Test Delta Live Tables Syntax
          - script: |
              python -c "
              import ast
              import os
              for root, dirs, files in os.walk('pipelines/databricks/delta-live-tables'):
                  for file in files:
                      if file.endswith('.py'):
                          filepath = os.path.join(root, file)
                          with open(filepath, 'r') as f:
                              try:
                                  ast.parse(f.read())
                                  print(f'✓ {filepath} syntax is valid')
                              except SyntaxError as e:
                                  print(f'✗ {filepath} syntax error: {e}')
                                  exit(1)
              "
            displayName: 'Validate Delta Live Tables Syntax'
          
          # Data Quality Tests
          - script: |
              cd pipelines/databricks/notebooks
              python -m pytest ../../../tests/data_quality/ -v --junitxml=test-results.xml
            displayName: 'Run Data Quality Tests'
            continueOnError: true
          
          # Publish Test Results
          - task: PublishTestResults@2
            condition: always()
            inputs:
              testResultsFormat: 'JUnit'
              testResultsFiles: 'test-results.xml'
              testRunTitle: 'Data Quality Tests'

      - job: AutomationValidation
        displayName: 'Automation Scripts Validation'
        pool:
          vmImage: 'ubuntu-latest'
        
        steps:
          - checkout: self
          
          # Setup Python
          - task: UsePythonVersion@0
            inputs:
              versionSpec: $(pythonVersion)
          
          # Install Dependencies
          - script: |
              pip install azure-functions azure-storage-blob azure-identity pytest pylint
            displayName: 'Install Automation Dependencies'
          
          # Lint Automation Scripts
          - script: |
              find automation/ -name "*.py" -exec pylint {} \; || true
            displayName: 'Lint Automation Scripts'
          
          # Test Automation Functions
          - script: |
              cd automation
              python -m pytest tests/ -v --junitxml=automation-test-results.xml
            displayName: 'Test Automation Functions'
            continueOnError: true
          
          # Publish Results
          - task: PublishTestResults@2
            condition: always()
            inputs:
              testResultsFormat: 'JUnit'
              testResultsFiles: 'automation-test-results.xml'
              testRunTitle: 'Automation Tests'

  # ===========================
  # DEVELOPMENT DEPLOYMENT
  # ===========================
  - stage: DeployDev
    displayName: 'Deploy to Development'
    dependsOn: BuildAndValidate
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/develop'))
    variables:
      - group: dev-variables
      - name: environment
        value: 'dev'
    
    jobs:
      - deployment: DeployInfrastructureDev
        displayName: 'Deploy Infrastructure to Dev'
        environment: 'dev-environment'
        pool:
          vmImage: 'ubuntu-latest'
        
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self
                
                # Install Terraform
                - task: TerraformInstaller@0
                  inputs:
                    terraformVersion: $(terraformVersion)
                
                # Azure Login
                - task: AzureCLI@2
                  displayName: 'Azure CLI Login'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: 'echo "Logged in to Azure"'
                
                # Terraform Init
                - task: AzureCLI@2
                  displayName: 'Terraform Init'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform init \
                        -backend-config="resource_group_name=$(tfStateResourceGroup)" \
                        -backend-config="storage_account_name=$(tfStateStorageAccount)" \
                        -backend-config="container_name=$(tfStateContainer)" \
                        -backend-config="key=dev.terraform.tfstate"
                
                # Terraform Plan
                - task: AzureCLI@2
                  displayName: 'Terraform Plan'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform plan \
                        -var-file="environments/dev/terraform.tfvars" \
                        -out=tfplan
                
                # Terraform Apply
                - task: AzureCLI@2
                  displayName: 'Terraform Apply'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform apply tfplan
                
                # Export Terraform Outputs
                - task: AzureCLI@2
                  displayName: 'Export Terraform Outputs'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform output -json > terraform-outputs.json
                      cat terraform-outputs.json
                
                # Publish Terraform Outputs as Pipeline Artifact
                - task: PublishPipelineArtifact@1
                  inputs:
                    targetPath: 'infrastructure/terraform/terraform-outputs.json'
                    artifactName: 'terraform-outputs-dev'

      - deployment: DeployDataPipelinesDev
        displayName: 'Deploy Data Pipelines to Dev'
        dependsOn: DeployInfrastructureDev
        environment: 'dev-environment'
        pool:
          vmImage: 'ubuntu-latest'
        
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self
                
                # Download Terraform Outputs
                - task: DownloadPipelineArtifact@2
                  inputs:
                    artifactName: 'terraform-outputs-dev'
                    targetPath: '$(Pipeline.Workspace)'
                
                # Setup Python and Databricks CLI
                - task: UsePythonVersion@0
                  inputs:
                    versionSpec: $(pythonVersion)
                
                - script: |
                    pip install databricks-cli
                  displayName: 'Install Databricks CLI'
                
                # Configure Databricks CLI
                - task: AzureCLI@2
                  displayName: 'Configure Databricks CLI'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      # Extract Databricks workspace URL from Terraform outputs
                      DATABRICKS_HOST=$(cat $(Pipeline.Workspace)/terraform-outputs.json | jq -r '.databricks.value.workspace_url')
                      
                      # Get Azure access token
                      AZURE_TOKEN=$(az account get-access-token --resource=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d --query accessToken -o tsv)
                      
                      # Configure Databricks CLI
                      echo "[DEFAULT]" > ~/.databrickscfg
                      echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
                      echo "token = $AZURE_TOKEN" >> ~/.databrickscfg
                      echo "auth_type = azure-cli" >> ~/.databrickscfg
                
                # Deploy Notebooks
                - script: |
                    databricks workspace import_dir pipelines/databricks/notebooks /Shared/pipelines/notebooks --overwrite
                  displayName: 'Deploy Databricks Notebooks'
                
                # Deploy Delta Live Tables Pipeline
                - script: |
                    # Create DLT pipeline configuration
                    cat > dlt-pipeline-config.json << EOF
                    {
                      "name": "medallion-architecture-pipeline-dev",
                      "storage": "/mnt/data-lake/dev/pipelines",
                      "configuration": {
                        "environment": "dev"
                      },
                      "clusters": [
                        {
                          "label": "default",
                          "autoscale": {
                            "min_workers": 1,
                            "max_workers": 2
                          }
                        }
                      ],
                      "libraries": [
                        {
                          "notebook": {
                            "path": "/Shared/pipelines/delta-live-tables/medallion_pipeline"
                          }
                        }
                      ],
                      "target": "dev_medallion_db",
                      "continuous": false
                    }
                    EOF
                    
                    # Deploy pipeline (would use Databricks API in real implementation)
                    echo "DLT Pipeline configuration created"
                    cat dlt-pipeline-config.json
                  displayName: 'Deploy Delta Live Tables Pipeline'

  # ===========================
  # PRODUCTION DEPLOYMENT
  # ===========================
  - stage: DeployProd
    displayName: 'Deploy to Production'
    dependsOn: BuildAndValidate
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
      - group: prod-variables
      - name: environment
        value: 'prod'
    
    jobs:
      - deployment: DeployInfrastructureProd
        displayName: 'Deploy Infrastructure to Production'
        environment: 'prod-environment'
        pool:
          vmImage: 'ubuntu-latest'
        
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self
                
                # Manual Approval Gate (configured in Azure DevOps Environment)
                
                # Install Terraform
                - task: TerraformInstaller@0
                  inputs:
                    terraformVersion: $(terraformVersion)
                
                # Terraform Init
                - task: AzureCLI@2
                  displayName: 'Terraform Init'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform init \
                        -backend-config="resource_group_name=$(tfStateResourceGroup)" \
                        -backend-config="storage_account_name=$(tfStateStorageAccount)" \
                        -backend-config="container_name=$(tfStateContainer)" \
                        -backend-config="key=prod.terraform.tfstate"
                
                # Terraform Plan with Production Settings
                - task: AzureCLI@2
                  displayName: 'Terraform Plan'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform plan \
                        -var-file="environments/prod/terraform.tfvars" \
                        -out=tfplan
                
                # Terraform Apply with Extra Validation
                - task: AzureCLI@2
                  displayName: 'Terraform Apply'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd infrastructure/terraform
                      terraform apply tfplan
                
                # Post-Deployment Validation
                - task: AzureCLI@2
                  displayName: 'Post-Deployment Validation'
                  inputs:
                    azureSubscription: $(azureSubscription)
                    scriptType: 'bash'
                    scriptLocation: 'inlineScript'
                    inlineScript: |
                      cd scripts/testing
                      python infrastructure_health_check.py --environment prod

      - deployment: DeployDataPipelinesProd
        displayName: 'Deploy Data Pipelines to Production'
        dependsOn: DeployInfrastructureProd
        environment: 'prod-environment'
        pool:
          vmImage: 'ubuntu-latest'
        
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self
                
                # Production Pipeline Deployment with Blue/Green Strategy
                - script: |
                    echo "Deploying production pipelines with blue/green strategy"
                    # Implementation would include blue/green deployment logic
                  displayName: 'Deploy Production Pipelines'
                
                # Post-Deployment Testing
                - script: |
                    cd scripts/testing
                    python end_to_end_pipeline_test.py --environment prod
                  displayName: 'End-to-End Pipeline Testing'

  # ===========================
  # MONITORING AND ALERTING
  # ===========================
  - stage: PostDeploymentMonitoring
    displayName: 'Post-Deployment Monitoring'
    dependsOn: 
      - DeployDev
      - DeployProd
    condition: always()
    
    jobs:
      - job: SetupMonitoring
        displayName: 'Setup Monitoring and Alerting'
        pool:
          vmImage: 'ubuntu-latest'
        
        steps:
          - checkout: self
          
          # Deploy Monitoring Dashboards
          - task: AzureCLI@2
            displayName: 'Deploy Monitoring Dashboards'
            inputs:
              azureSubscription: $(azureSubscription)
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                cd monitoring/dashboards
                for dashboard in *.json; do
                  az portal dashboard import --input-path "$dashboard"
                done
          
          # Setup Automated Health Checks
          - script: |
              cd automation/azure-functions
              python deploy_health_checks.py
            displayName: 'Deploy Health Check Functions'
          
          # Validate Monitoring Setup
          - script: |
              cd scripts/testing
              python monitoring_validation.py
            displayName: 'Validate Monitoring Setup'
